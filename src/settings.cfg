[DEFAULT]
active_llm = Ollama
model = llama3:latest 
messy = False
max_items = 10

[Ollama]
url = http://localhost:11434
endpoint = /api/generate
model = llama3:latest 
max_tokens = 300
temperature = 0.7
stream = False
models_endpoint = /api/tags
models_key = models
model_name_key = name
payload_structure = {"model": "model", "prompt": "prompt", "stream": "stream"}
response_key = response
messy = False
max_items = 10

[LM Studio]
url = http://localhost:1234
endpoint = /v1/chat/completions
model = gpt-3.5-turbo
max_tokens = -1
temperature = 0.7
system_message = "Understood. Transitioning to requested role and context."
models_endpoint = /v1/models
models_key = data
model_name_key = id
payload_structure = {"model": "model", "messages": [{"role": "system", "content": "system_message"}, {"role": "user", "content": "prompt"}]}
response_key = choices.0.message.content

[GPT4All]
url = http://localhost:4891
endpoint = /v1/completions
model = gpt4all-j-v1.3-groovy
max_tokens = 2000
temperature = 0.7
top_p = 0.95
models_endpoint = /v1/models
models_key = data
model_name_key = id
payload_structure = {"model": "model", "prompt": "prompt"}
response_key = choices.0.text

[LocalAI]
url = http://localhost:8080
endpoint = /v1/chat/completions
model = mistral-7b-instruct-v0.3
max_tokens = 2000
temperature = 0.7
top_p = 0.95
stream = false
models_endpoint = /v1/models
models_key = data
model_name_key = id
payload_structure = {"model": "model", "messages": [{"role": "user", "content": "prompt"}]}
response_key = choices.0.message.content

[llama.cpp]
url = http://localhost:8080
endpoint = /completion
max_tokens = 2000
temperature = 0.7
top_p = 0.95
stop = </s>,Human:,AI:
payload_structure = {"prompt": "prompt", "n_predict": "max_tokens", "temperature": "temperature", "stop": "stop"}
response_key = content

[oobabooga]
url = http://localhost:5000
endpoint = /v1/chat/completions
max_new_tokens = 2000
temperature = 0.7
top_p = 0.95
stop = </s>,Human:,AI:
max_tokens = 300
stream = false
models_endpoint = /api/v1/model
models_key = model_names
payload_structure = {"messages": [{"role": "user", "content": "prompt"}], "max_tokens": "max_tokens", "temperature": "temperature", "top_p": "top_p", "stop": "stop"}
response_key = choices.0.message.content

#rough guesses:

[ChatGPT]
url = https://api.openai.com
endpoint = /v1/chat/completions
model = gpt-3.5-turbo
max_tokens = 1000
temperature = 0.7
top_p = 1.0
models_endpoint = /v1/models
models_key = data
model_name_key = id
payload_structure = {"model": "model", "messages": [{"role": "system", "content": "system_message"}, {"role": "user", "content": "prompt"}]}
response_key = choices.0.message.content

[Perplexity]
url = https://api.perplexity.ai
endpoint = /v1/chat/completions
model = pplx-7b-chat
max_tokens = 1000
temperature = 0.7
top_p = 1.0
models_endpoint = /v1/models
models_key = data
model_name_key = id
payload_structure = {"model": "model", "messages": [{"role": "user", "content": "prompt"}]}
response_key = choices.0.message.content

[Claude]
url = https://api.anthropic.com
endpoint = /v1/messages
model = claude-3-5-sonnet-20240620
max_tokens = 1000
temperature = 0.7
top_p = 1.0
models_endpoint = /v1/models
models_key = data
model_name_key = id
payload_structure = {"model": "model", "messages": [{"role": "user", "content": "prompt"}]}
response_key = choices.0.message.content

[Gemini]
url = https://generativelanguage.googleapis.com
endpoint = /v1/models/gemini-1.5-pro:generateContent
model = gemini-1.5-pro
max_tokens = 1000
temperature = 0.7
top_p = 1.0
models_endpoint = /v1/models
models_key = data
model_name_key = id
payload_structure = {"model": "model", "messages": [{"role": "user", "content": "prompt"}]}
response_key = choices.0.message.content

