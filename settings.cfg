[DEFAULT]
active_llm = Ollama
current_model = mistral:latest
messy = False
max_items = 5

[Ollama]
url = http://localhost:11434
endpoint = /api/generate
model = llama2
max_tokens = 2000
temperature = 0.7

[LM Studio]
url = http://localhost:1234
endpoint = /v1/chat/completions
model = gpt-3.5-turbo
max_tokens = -1
temperature = 0.7
system_message = "Understood. Transitioning to requested role and context."
#longer system_message for those with cheaper token prices
#system_message = "Acknowledged. Preparing to assume the requested persona and contextual framework as directed."
#even longer
#system_message = "Acknowledged the directive. Initiating transition to embody the specified persona, role, and contextual parameters as outlined in the forthcoming prompt. Awaiting further instructions to proceed accordingly."

[GPT4All]
url = http://localhost:4891
endpoint = /v1/completions
model = gpt4all-j-v1.3-groovy
max_tokens = 2000
temperature = 0.7
top_p = 0.95

[LocalAI]
url = http://localhost:8080
endpoint = /v1/chat/completions
model = gpt-3.5-turbo
max_tokens = 2000
temperature = 0.7
top_p = 0.95

[llama.cpp]
url = http://localhost:8080
endpoint = /completion
max_tokens = 2000
temperature = 0.7
top_p = 0.95
stop = </s>,Human:,AI:

[oobabooga]
url = http://localhost:5000
endpoint = /api/v1/generate
max_new_tokens = 2000
temperature = 0.7
top_p = 0.95
stop = </s>,Human:,AI:

