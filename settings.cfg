[DEFAULT]
active_llm = Ollama
current_model = mistral:latest
messy = False
max_items = 5

[Ollama]
model = llama2
max_tokens = 200
temperature = 0.7
stream = False
llm_name = Ollama

[LM Studio]
model = gpt-3.5-turbo
max_tokens = 100
temperature = 0.7
top_p = 1.0

[GPT4All]
model = gpt4all-j-v1.3-groovy
max_tokens = 100
temperature = 0.7
top_p = 0.95

[LocalAI]
model = gpt-3.5-turbo
max_tokens = 100
temperature = 0.7
top_p = 1.0

[llama.cpp]
max_tokens = 100
temperature = 0.7
top_p = 0.95
stop = ["\n", "Human:", "AI:"]

[oobabooga]
max_new_tokens = 100
temperature = 0.7
top_p = 0.95
stop = ["\n", "Human:", "AI:"]

