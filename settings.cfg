[DEFAULT]
active_llm = LocalAI
model = mistral-7b-instruct-v0.3
messy = False
max_items = 5

[Ollama]
url = http://localhost:11434
endpoint = /api/generate
model = llama2
max_tokens = 2000
temperature = 0.7

[LM Studio]
url = http://localhost:1234
endpoint = /v1/chat/completions
model = gpt-3.5-turbo
max_tokens = -1
temperature = 0.7
system_message = "Understood. Transitioning to requested role and context."

[GPT4All]
url = http://localhost:4891
endpoint = /v1/completions
model = gpt4all-j-v1.3-groovy
max_tokens = 2000
temperature = 0.7
top_p = 0.95

[LocalAI]
url = http://localhost:8080
endpoint = /v1/chat/completions
model = mistral-7b-instruct-v0.3
max_tokens = 2000
temperature = 0.7
top_p = 0.95
stream = True
messy = False
max_items = 5

[llama.cpp]
url = http://localhost:8080
endpoint = /completion
max_tokens = 2000
temperature = 0.7
top_p = 0.95
stop = </s>,Human:,AI:

[oobabooga]
url = http://localhost:5000
endpoint = /api/v1/generate
max_new_tokens = 2000
temperature = 0.7
top_p = 0.95
stop = </s>,Human:,AI:

